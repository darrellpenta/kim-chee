---
title: "Experiment 3"
author: "Darrell Penta"
date: "April 3, 2015"
output: html_document
---

***
## I. EXP OVERVIEW
### a. What you did 
You saw a grid of letters (6 columns and 20 rows), and before seeing these, a target letter was shown and you were supposed to look through the grid top to bottom and we measured the time it took find the target.  We measured __RT__ as the time it took to find target in the midst of distractors

This is a visual search task:

* doesn't depend on working memory
* finding a visual representation in the midst of other visual representations
* it's a pattern recognition process

We do this all the time. _Finding a friend in a crowd; finding the match to your sock on the floor of your bedroom_

### b. Experimental questions
* _What are visual patterns like?_ _How are they stored?_ 
    + These are questions about representations. 

*  _What's the process like?_ 
    + "Test process" (the overall process was enforced, which was to look serially from top to bottom, so we want to know about the test instead-- that is, how does the matching of visual input against a stored pattern work)

* _How similar is target to the distractor?_
    + (VWYK), (CDGO) vs. Z and Q
    
* _How is similarity defined?_ _Over what properties?_ _What are the kinds of things that matter for visual similarity?_

***

## II. BACKGROUND


### a. The template Theory
* Visual patterns are overlays
* They're like cookie cutters
* They have to be good enough to match the desired cases and to reject the others
* must be able to:
      + align
      + rotations
      + scale them

* This get complicated with three dimensional objects; This is a problem for vision in general, since mapping 3 dimension to the retina (two dimensions) means loosing a dimension. 

#### __Phillips (1972)__
* Iconic memory (or very short term visual buffer)
    * gave Ps a grid-based dot pattern, then a delay, and then another grid
    * Ps were asked whether they matched (sometimes it was the same grid, other times, it was the same pattern but shifted relative to the original grid
    * Used various inter-stimulus-intervals (20, 60, 100, 300, 600ms)
    * compared exact matches so shifted cases

People are very accurate for match cases from 20ms (100% correct) to about 100s, then closer to chance (60%); for shifted cases, people just hover around 60% correct. Phillips suggests that there is a template system available for use in very short-term visual tasks
(note: early fingerprint and face recognition systems used template matching)

 ![](https://miltontan.files.wordpress.com/2010/05/the-cat.jpg) 
 
 * Template theory needs to be able to explain why the H-A are different. 
 * Visual ambiguity is not well-handled in this theory


The feature theory
---------------------
-Visual patters are collections of features
-align, rotate, and scale over all features simultaneously

-Features can be shared across objects;  you can count overlapping features. 
Ex> Q and O, are similar based on feature overlap: (,),~ vs. (,) 

GIBSON (1969)
Discusses good sets of features for given domains
_visual Features need to be critical-- they must differentiate among representations
_Visual features should be invariant over a variety of changes in brightness, size, perspective
_unique pattern for each thing to be recognized. 
_Feature set should be small relative to the things in the domain

Evidence:
If you highlight features that differentiate each other, kids can learn more easily (E vs. F)

Gibson, Shapiro, Yonas (1968)
-Adults saw pairs of letters, and had to decide whether letters were same or difference
(Hyp. people would confuse them if they were similar): {{{M-N},W}, {E-F}} vs. CG, PR

Results split as curvy vs. straight features; 

Features theories also have trouble with "THE CAT"-- 
--you can add information about how the paces fit together-- how they map onto each other. 

## Structural feature
Context | Letters
--------|--------
VWYK	  |		CDGO


TEMPLATE PREDICTIONS:
1. Q or Z might bet harder to to find, or not (Neither theory cares)
2. Angular might be harder than rounded, or vice versa (or not)
**But, since context and target vary, there are different predictions, since on 25% of the trials, a rounded letter will be in a round context, and also, angular in angled context:
__Q<sub>ROUND</sub>__, __Q<sub>ANGLE</sub>__, __Z<sub>ROUND</sub>__, __Z<sub>ANGLE</sub>__
3. Template just matches template serially over target; speed is not influenced by context, since similarity is not discernible in Templ. Theory.

FEATURE PREDICTIONS:
1. Q or Z might bet harder to to find, or not (Neither theory cares)
2. Angular might be harder than rounded, or vice versa (or not)
3. Feature similarity determines amount of match (= difficult)
---Targets are slower in ambiguous context
Partial matches ( e.g., "(" when looking for "(,),~") will slow the system down

__Q<sub>ROUND</sub>__, lots of overlap
__Q<sub>ANGLE</sub>__, very little
__Z<sub>ROUND</sub>__, no overlap
__Z<sub>ANGLE</sub>__, maybe a ltitle

<div style="background:#fff0f5">
## Abstract (10) 
* Describe template and feature theories briefly; describe the task; describe predictions and results, focusing on interaction between context and target (no reason to discuss row effects)

## Intro (20)
* General introductory paragraph w/an everyday example; 
* Description of __template theory__ w/examples of use (e.g., [fingerprint ID, face recognition, -- SKIPPED in Spr2015] 
    * iconic memory - Phillips expt, other examples from readings;
    * Phillips is only one we ended up talking about in class in any detail) and presentation of advantages (simple) and problems (e.g., need for multiple templates, difficulty handling variation in wide-ranging domains, difficulty with various transformations, difficulty with ambiguity);
* description of __feature theory__ w/examples of use (e.g., letter features) and some discussion of supporting studies (lots of possibilities from the reading), advantages (simpler to store representations), and problems (e.g., difficulty with some transformations, difficulty with ambiguity);
    * brief description of experimental task; predictions of template theory, describing process of scanning (overlay template successively): no interaction between target & context, no prediction about main effects;
    * predictions of feature theory, describing process (look for features, so more feature overlap between target and context yields longer RTs): no prediction about main effects, predicts interaction (Qrnd>Qang, Zrnd<=Zang) -- predictions should focus on interaction presence/absence
* quantitative stuff about % overlap from features can go here, but most likely in the discussion
* __structural feature theory__  can also be described in intro (after template and feature theories, but probably before presentation of the particlar expt) or in discussion; it should be in at least one of intro & discussion
</div>

## II. METHODS

### Participants:
16 NU Undergrads

### Apparatus:
MicroExperimental lab sortware running DOS

### Materials:
2 target letters, q or z, and context letters, VNYK, angular, and CDGO, all capitals, 6 col by 20 row grid

### Procedure: 
#### Overall: 
Instructions on screen at beginning about how to do trial, one practice trial, 160 experimental trials, 20 minutes
#### Individual: 
Target letter displayed (q or z), press return when ready, grid appears on screen, do search, and press space when found; Y/N confirmation if thought trials was done correctly. 

### Design:

#### IVS
1. Target letter 
    * two levels (Q, Z)
2. context
    * two levels (Rounded, angular)
3. Row --in which target appears, targets presented equally often in each row
    * 20 levels, 1-20
4. Repetition- two repetitions of each combination of the other three independent variables

* All IVs manipulated within Ps

* 2 target X 2 context X 20 rows x 2 repetition = 160 trials

#### DVs
1. Reaction time, measured from when grid appears to when space bar was pressed

<div style="background:#fff0f5">
## Method

### Participants (2)
  * 16 Ss and source

### Apparatus (2)
  * Four personal computers runnin _MicroExperimental Lab_ (Schneider, 1988) on DOS

### Materials (3)
  * letters for targets (Q, Z) and for context sets (rounded: CDGO, angular: VWYK); 
  * 6 column x 20 row grid (grid can be described either here or in procedure)

### Procedure (4)

#### Indiv trial structure
  * target letter displayed, 
  * press <Enter>, 
  * grid appears, 
  * scan for target, 
  * press space bar, 
  * confirm (Y/N) if trial is OK

#### Overall expt:  
  * instructions,
  * 1 practice trial,
  * 160 trials;
  * total time approx 25(?) min [should specify an approx time, but don't really care exactly what it is]

### Design (4)
  * 4 IVs:  row, target, context, repetition -- name each one, describe each one, list levels (describe levels if necessary -- e.g., context letters if not already detailed in materials); 
  * all IVs manip w/in Ss
  * 1 DV:  reaction time from grid appearance until space bar press
</div>



***

## ANOVA
### Main Effects (in factor, comparing levels to eachother) 
* e.g., is q faster than z  (is one level of target faster than the other?) This is a t-test when there's only two levels---- looks at the differences in two means relative to variation for each level
* Collapsing over all other IVS
* We're looking for a target and context main effect

### Interactions 
* We have 2 IVs, so we'll be looking for combinations for IV<sub>1</sub> vs. IV<sub>2</sub>
* Feature theory cares about interaction of target and context:
    + __Q<sub>ROUND</sub>__ > __Q<sub>ANGLE</sub>__
    + __Z<sub>ROUND</sub>__ <= __Z<sub>ANGLE</sub>__
* Parallel lines mean no interaction

***
## Results
* No data were thrown out
* We filtered out trials in which the participant indicated that they had done the trial incorrectly
### Context by Target Analyses
#### Collapsing

* Collapsed over repetition by taking means
* Collapsed over row by taking means 
    + this give us four values for each person, __Q<sub>ROUND</sub>__, Qand, __Z<sub>ROUND</sub>__, __Z<sub>ANGLE</sub>__
* Collapsed over participants by taking means    

> Students need to present **target** by **context** means in a figure
 Report patterns 
 Report anova stats

#### ANOVA Results
We ran a 2 (**Target**) X 2 (**Context**) ANOVA

> Neal explains *degrees of freedom*: If you know the mean RT is 4150, and you know that one of the means is, for example, Q=4300, we can compute the other as 4000. 

* We did not find a ME effect of __Target__: letter (F(1,15) = 2.19, _p_ > .15)
* We did not find a ME effect of __Context__: (F(1,15) = 1.71, _p_ > .20)
* We _DID_ find an interaction between __Target__ and __Context__: (F(1,15) = 77.8, _p_ < .001)

### Row analyses

#### Collapsing

* Collapsed over repetition by taking means
* Collapsed over target and context by taking means 
    + this give us an overall R<sub>n</sub> mean for each person
* Collapsed over participants, giving us a set of row means

If people are paying attention to the instructions, there should be larger reaction times as the row number increases.

"Reaction Time by Row" plot
Reason Time (ms)
Row


```{r, echo=FALSE, error=FALSE, warning=FALSE}
library(ggplot2)

rt.by.row <- data.frame(
Row = c(1:20),
RT = c(2120,2011,2431,2545,3183,3126,3350,3499,3825,4217,4150,4672,4902,5007,5234,5775,5722,5936,6049,6021)
)
model <- lm(formula = RT ~ Row, data = rt.by.row)
eqn <- as.character( as.expression(
substitute(italic(y) == a + b * italic(x) * "," ~~ italic(r)^2 ~ "=" ~ r2,
           list(a = format(coef(model)[1], digits = 3),
                b = format(coef(model)[2], digits = 3),
                r2 = format(summary(model)$r.squared, digits=2)
                ))))
```


```{r, echo=FALSE}

pl<-ggplot(rt.by.row, aes(x = Row, y = RT))
pl + geom_point() + geom_smooth(method = lm, se = FALSE) +  theme_classic() + scale_x_continuous(breaks=c(1:20)) +
  annotate("text", label = eqn, parse = TRUE, x = Inf, y = -Inf, hjust = 1.1, vjust = -.5)

```

We have a slope: y(RT) = 229x(row) + 1779

What does this tell us (that it take approximate. 229ms/row to scan one row)

You also want a Pearson R value:
* r=.99
* _p_<.001


<div style="background:#fff0f5">
## Results (15)

* dropped any trials where confirmation=<N>

1. factorial (target x context) analysis: 
  * collapsing (taking means) over row, repetition, and participants; 
  * describe pattern (Qrnd > Qang, Zrnd < Zang);
  * refer to fig; present ANOVA (no main eff of target, Q > Z (F(1,15)=2.19, p>.15); 
  * no main eff of context (F(1,15)=1.71, p>.20);
  * interaction, Qrnd>Qang, Zrnd<Zang: F(1,15)=77.8, p<.001)

2. row analysis: 
  * collapsing (taking means) over repetition, target, context, and participants; 
  * describe pattern (RT increasing with row);
  * refer to fig; present regression (slope = 229 ms/row, y-int = 1779 ms, r = 0.99, p < .001);
  * brief descrip of fit of line -- very good across the range [Spr2015 -- skipped this entirely; better for first 10 rows or so than for rest [could be here or in Discussion]]
(either order of above analyses is fine)
</div>
***
Feature Theory (Talked about in Intro):

* Knows and cares about _amount_ of overlap
* computes this overlap by comparing number of features shared (e.g., Q vs. D): (, ), ~ vs. ), |
* more shared features means slower reaction times
  + Q<sub>ROUND</sub> > Q<sub>ANGULAR</sub>
  + Z<sub>ROUND</sub> &le; Z<sub>ANGULAR</sub>
  
  # Discussion
* Template theory doesn't have a way of talking about anything other than overall overlap
* It doesn't say anything about combinations (It shouldn't know that a Q overlapped with a D has some overlap with a Q overlapped with a V)
* Predicts no interaction between target and context
* Specific pattern of results matches the feature theory predictions
* WE can define a specific set of features for the relevant letters, and we can define a function that computes amount of overlap
    + we have a set of features counted up (see table
    
    

$\[%overlap= # of matching features/# of features in context\]$  
  (% overlap, # features matching / # of features in context =  Q<sub>ROUND</sub> 2/5=40%,, q<sub>ANGULAR</sub> = 1/8 = 12.5%
      + % overlap, # features matching / # of features in context =  Z<sub>ROUND</sub> 0/5=0%, Z<sub>ANGULAR</sub> = 1/8 = 12.5%
    
Bose we don't see a perfect match in the computations and the results, there's more to the story 
-potential problem: we're not considering how many times each of the features appear, we might want to adjust the function to weight features

Letter | Features | No. of Features
---------------------------------
Z      | ,/,- | 3

  

We found an interaction, which fits with the feature theory but does not fit with the template theory
-What might you do to the template theory to explain the patter?
-    

# STructural Feature Theory
-Pure feature theory doesn't consider relative positioning of features

Discussion (15)

<div style="background:#fff0f5">
* compare results w/predictions (recap predictions if needed)
  * no effect of target or context, compatible w/either theory, but interaction inconsistent w/Template and consistent w/Feature; 
* explain meaning of slope (time to scan a row) [and mention goodness of fit to regression line for earlier vs later rows (optional; should be here or in Results; nothing to do for Spr2015)];

* discuss augmentation of Template;

* discuss Structural Feature Theory (if not covered in Intro);

* cover detailed feature theory and quantitative predictions here, relating to ANOVA results, etc. (some flexibility about how much they say about quantitative feature predictions, but they should present a feature-breakdown and work out % of overlap, relating to results, at a minimum); 

* usual general issues & extensions


## FIGS AND TABLES (15)

graph for RT by target and context

graph for RT by row, including regression line

check axis labels, axis tick spacing, graph titles, etc.

</div>



***
### READINGS:
* Levy, C.M., & Ransdell, S. E. (1988). _Laboratory in cognition and perception (2nd ed.)._ Iowa City: CONDUIT, Univ. of Iowa.
* Neisser, U. (1967) _Cognitive Psychology_. Englewood Cliffs, NJ: Prentice-Hall. [Neisser (1967)](http://infinitychallenge.com/articles/Neisser_1967.pdf)
* Reed, S.K. (1998) _Cognition: Theory and appliciations. Pacific Grove, CA: Brooks/Cole. [Reed (1988)](http://infinitychallenge.com/articles/Reed_1967.pdf)
* Schneider, W. (1988) Micro Experimental Laboratory: An integrated system for IBM PC compatibles: _Behavior Research Methods, Instruments, & Computers, 20_, 206-217.

```{r}
library(languageR) 
library(reshape2)
library(plyr)
library(stringr)
# target: 1=Q, 2=Z
# context: 1=rnd, 2=ang
# row: 1-20
# Rep
# 1\2\12\1
class.dat.base <- read.table("data/EXP3.txt", header = T) 
colnames(class.dat.base) <- c("subj", "trial", "target", "context", "row", "rep","RT","OK")
class.dat    <- subset(class.dat.base, target != 1 & context != 2 & row != 12 & rep != 1 & OK != 2)


# # CAT: Collapse over subject
# d.cat$unique.id <- paste(d.cat[, 1], d.cat[, 3], d.cat[, 4], d.cat[, 5], sep = "_")
# d.cat           <- ddply(d.cat, "unique.id", function(X) data.frame(vtime = mean(X$vtime)))
# vtime           <- data.frame(d.cat$vtime)
# d.cat           <- colsplit(d.cat$unique.id, "_", c("subj", "subexp", "headrel", "localrel"))
# d.cat$subj      <- as.factor(d.cat$subj)
# d.cat           <- cbind(d.cat,vtime)
# colnames(d.cat)[5] <- "vtime"
# rm(vtime)
```

