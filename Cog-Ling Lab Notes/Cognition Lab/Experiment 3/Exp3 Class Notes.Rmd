---
title: "Experiment 3"
author: "Darrell Penta"
date: "April 3, 2015"
output: html_document
---

***
## I. EXP OVERVIEW
### a. What you did 
You saw a grid of letters (6 columns and 20 rows), and before seeing these, a target letter was shown and you were supposed to look through the grid top to bottom and we measured the time it took find the target.  We measured __RT__ as the time it took to find target in the midst of distractors

This is a visual search task:

* doesn't depend on working memory
* finding a visual representation in the midst of other visual representations
* it's a pattern recognition process

We do this all the time. _Finding a friend in a crowd; finding the match to your sock on the floor of your bedroom_

### b. Experimental questions
* _What are visual patterns like?_ _How are they stored?_ 
    + These are questions about representations. 

*  _What's the process like?_ 
    + "Test process" (the overall process was enforced, which was to look serially from top to bottom, so we want to know about the test instead-- that is, how does the matching of visual input against a stored pattern work)

* _How similar is target to the distractor?_
    + (VWYK), (CDGO) vs. Z and Q
    
* _How is similarity defined?_ _Over what properties?_ _What are the kinds of things that matter for visual similarity?_

***

## II. BACKGROUND

<div style="border: 1px solid #000000; padding: 5px">

### a. The template theory
* Visual patterns are overlays
* They're like cookie cutters
* They have to be good enough to match the desired cases and to reject the others
* must be able to:
      + align
      + rotations
      + scale them

* This get complicated with three dimensional objects; This is a problem for vision in general, since mapping 3 dimension to the retina (two dimensions) means loosing a dimension. 

#### __Phillips (1974)__
* Iconic memory (or very short term visual buffer)
    * gave Ps a grid-based dot pattern, then a delay, and then another grid
    * Ps were asked whether they matched (sometimes it was the same grid, other times, it was the same pattern but shifted relative to the original grid
    * Used various inter-stimulus-intervals (20, 60, 100, 300, 600ms)
    * compared exact matches so shifted cases

People are very accurate for match cases from 20ms (100% correct) to about 100s, then closer to chance (60%); for shifted cases, people just hover around 60% correct. Phillips suggests that there is a template system available for use in very short-term visual tasks
(note: early fingerprint and face recognition systems used template matching)

 ![](https://miltontan.files.wordpress.com/2010/05/the-cat.jpg) 
 
 * Template theory needs to be able to explain why the H-A are different. 
 * Visual ambiguity is not well-handled in this theory

#### PREDICTIONS:
1. Q or Z might bet harder to to find, or not (Neither theory cares)
2. Angular might be harder than rounded, or vice versa (or not)
**But, since context and target vary, there are different predictions, since on 25% of the trials, a rounded letter will be in a round context, and also, angular in angled context:
__Q<sub>ROUND</sub>__, __Q<sub>ANGLE</sub>__, __Z<sub>ROUND</sub>__, __Z<sub>ANGLE</sub>__
3. Template just matches template serially over target; speed is not influenced by context, since similarity is not discernible in Templ. Theory.
</div>


<div style="background:#ccffff;border: 1px solid #000000; padding: 5px; margin: 10px 0px 10px 25px">
#### Notes from Reed (1988) 
#### Template Theory
* templates are holistic and unanylzed
* we compare by measuring overlap
    * problematic becase template would need to be in same position and orientation
    * lots of variablity in patterns
    * can't describe the difference between two patterns (e.g., can't say why _R_ differs from _P_ because no access to features)
    * alternative descriptions of a pattern are not allowed (e.g., a sail vs. a stringray)
* can't totally be dismissed though
    * sensory store (Neal calls it _iconic memory_) briefly preserves information so that the observer has more time to recognize a pattern, so patterns must be preserved there if they are to be recognizes
    
##### Philliips (1974)    
* showed subjects patterns made by randomly filling cells in a matrix for 1 sec, followed at varying intervals by a second identical or similar pattern
* half the time, the second pattern was in same location, half the time it was shifted horizontally by one cell
    * if template matching is used, 2nd pattern in same location would be easier than shifted pattern
* Ss decide whether pattern was same or different
* % correct for match cases drops quickly, suggesting iconic memory has a short duration; % correct for mis-match cases was never great, suggesting Ss were recalling the pattern from visual memory, not from iconic memory
</div>

<div style="border: 1px solid #000000; padding: 5px">
### b. The feature theory
* Visual patterns are collections of features
* align, rotate, and scale over all features simultaneously
* Features can be shared across objects;  you can count overlapping features. 
    * Ex: Q and O, are similar based on feature overlap: (,),~ vs. (,) 

#### Gibson (1969)
* Discusses good sets of features for given domains
    * visual Features need to be critical-- they must differentiate among representations
    * Visual features should be invariant over a variety of changes in brightness, size, perspective
    * unique pattern for each thing to be recognized. 
    * Feature set should be small relative to the things in the domain
*Evidence: If you highlight features that differentiate each other, kids can learn more easily (E vs. F)

#### Gibson, Shapiro, Yonas (1968)
* Adults saw pairs of letters, and had to decide whether letters were same or difference
    * (Hyp. people would confuse them if they were similar): {{{M-N},W}, {E-F}} vs. CG, PR
    * Results split as curvy vs. straight features; 

* Features theories also have trouble with "THE CAT"-- 
    * you can add information about how the paces fit together-- how they map onto each other. 

#### PREDICTIONS:
1. Q or Z might bet harder to to find, or not (Neither theory cares)
2. Angular might be harder than rounded, or vice versa (or not)
3. Feature similarity determines amount of match (= difficult)
---Targets are slower in ambiguous context
Partial matches ( e.g., "(" when looking for "(,),~") will slow the system down

__Q<sub>ROUND</sub>__, lots of overlap
__Q<sub>ANGLE</sub>__, very little
__Z<sub>ROUND</sub>__, no overlap
__Z<sub>ANGLE</sub>__, maybe a ltitle
</div>

<div style="background:#ccffff;border: 1px solid #000000; padding: 5px;  margin: 10px 0px 10px 25px">
#### Notes from Reed (1988)
#### Feature Theory
* Describe patterns by listing parts

##### Gibson (1969) 
* children learn object identification by discriminating one object from another-- using _distinctive features_
* Egeland (1975) used color differences to highlight differences between letters (e.g., _G_ vs. _C_, _M_ vs. _N_); children trained on the distinctive features made fewer errors during repeat testing
* Gibson's criteria are listed

##### Gibson, Schapiro, & Yonas (1968)
* Ss (college students and 7-year-olds) saw pairs of letters and had to indicate whether they were the same or different
* two sets of nine leters were used
* They speculated that similar letters would slow reaction times and confirmed it
* Analyzed results by _hierarchically clustering_ letters into categories based on their features
    * found that straight/curved contrast was best predictor of perceived similarity
</div>


<div style="border: 1px solid #000000; padding: 5px">
## Structural feature
* Pure feature theory doesn't consider relative positioning of features


Context | Letters
--------|--------
VWYK	  |		CDGO

</div>

<div style="background:#ccffff;border: 1px solid #000000; padding: 5px;  margin: 10px 0px 10px 25px">
#### Notes from Reed (1988)
#### Structural Feature Theory
* Critical question is how features relate to each other, so making the relations more explicit is the goal of structural theories
* Grouping the parts can help determine whether the patter is a _sail_ or a _stingray_
* Start by listing features; then, specify the relation
* Lockhead & Crist (1980) used a card sorting task with children
    * kindergarten children sort decks into two bins
    * decks had either normal letters (e.g., _b_, _p_) or letters marked with dots
    * children were much faster and less error-prone with dotted letters

</div>
*** 
<div style="background:#fff0f5; border: 2px solid #990000">
## Abstract (10) 
* Describe template and feature theories briefly; describe the task; describe predictions and results, focusing on interaction between context and target (no reason to discuss row effects)

## Intro (20)
* General introductory paragraph w/an everyday example; 
* Description of __template theory__ w/examples of use:
    * (e.g., [fingerprint ID, face recognition, -- SKIPPED in Spr2015] 
* Iconic memory - Phillips expt, other examples from readings (Phillips is only one we ended up talking about in class in any detail)  
  * Presentation of advantages (simple) and problems (e.g., need for multiple templates, difficulty handling variation in wide-ranging domains, difficulty with various transformations, difficulty with ambiguity);
* Description of __feature theory__ w/examples of use (e.g., letter features) and some discussion of supporting studies (lots of possibilities from the reading)
* advantages (simpler to store representations), and problems (e.g., difficulty with some transformations, difficulty with ambiguity);

* Brief description of experimental task; 
* Predictions of __template theory__, describing process of scanning (overlay template successively): no interaction between target & context, no prediction about main effects;
* Predictions of __feature theory__, describing process (look for features, so more feature overlap between target and context yields longer RTs): no prediction about main effects, predicts interaction (Qrnd>Qang, Zrnd<=Zang) -- predictions should focus on interaction presence/absence

In intro or discussion:

* quantitative stuff about % overlap from features can go here, but most likely in the discussion
* __structural feature theory__  can also be described in intro (after template and feature theories, but probably before presentation of the particlar expt) or in discussion; it should be in at least one of intro & discussion
</div>

***
<div style="border: 1px solid #000000; padding: 5px">
## II. METHOD

### Participants:
16 NU Undergrads

### Apparatus:
MicroExperimental lab sortware running DOS

### Materials:
2 target letters, q or z, and context letters, VNYK, angular, and CDGO, all capitals, 6 col by 20 row grid

### Procedure: 
#### Overall: 
Instructions on screen at beginning about how to do trial, one practice trial, 160 experimental trials, 20 minutes
#### Individual: 
Target letter displayed (q or z), press return when ready, grid appears on screen, do search, and press space when found; Y/N confirmation if thought trials was done correctly. 

### Design:

#### IVS
1. Target letter 
    * two levels (Q, Z)
2. context
    * two levels (Rounded, angular)
3. Row --in which target appears, targets presented equally often in each row
    * 20 levels, 1-20
4. Repetition- two repetitions of each combination of the other three independent variables

* All IVs manipulated within Ps

* 2 target X 2 context X 20 rows x 2 repetition = 160 trials

#### DVs
1. Reaction time, measured from when grid appears to when space bar was pressed
</div>

<div style="background:#fff0f5;border: 2px solid #990000">
## Method

### Participants (2)
  * 16 Ss and source

### Apparatus (2)
  * Four personal computers runnin _MicroExperimental Lab_ (Schneider, 1988) on DOS

### Materials (3)
  * letters for targets (Q, Z) and for context sets (rounded: CDGO, angular: VWYK); 
  * 6 column x 20 row grid (grid can be described either here or in procedure)

### Procedure (4)

#### Indiv trial structure
  * target letter displayed, 
  * press <Enter>, 
  * grid appears, 
  * scan for target, 
  * press space bar, 
  * confirm (Y/N) if trial is OK

#### Overall expt:  
  * instructions,
  * 1 practice trial,
  * 160 trials;
  * total time approx 25(?) min [should specify an approx time, but don't really care exactly what it is]

### Design (4)
  * 4 IVs:  row, target, context, repetition -- name each one, describe each one, list levels (describe levels if necessary -- e.g., context letters if not already detailed in materials); 
  * all IVs manip w/in Ss
  * 1 DV:  reaction time from grid appearance until space bar press
</div>



***

## ANOVA
### Main Effects (in factor, comparing levels to eachother) 
* e.g., is q faster than z  (is one level of target faster than the other?) This is a t-test when there's only two levels---- looks at the differences in two means relative to variation for each level
* Collapsing over all other IVS
* We're looking for a target and context main effect

### Interactions 
* We have 2 IVs, so we'll be looking for combinations for IV<sub>1</sub> vs. IV<sub>2</sub>
* Feature theory cares about interaction of target and context:
    + __Q<sub>ROUND</sub>__ > __Q<sub>ANGLE</sub>__
    + __Z<sub>ROUND</sub>__ <= __Z<sub>ANGLE</sub>__
* Parallel lines mean no interaction

***
## Results
* No data were thrown out
* We filtered out trials in which the participant indicated that they had done the trial incorrectly
### Context by Target Analyses
#### Collapsing

* Collapsed over repetition by taking means
* Collapsed over row by taking means 
    + this give us four values for each person, __Q<sub>ROUND</sub>__, Qand, __Z<sub>ROUND</sub>__, __Z<sub>ANGLE</sub>__
* Collapsed over participants by taking means    

> Students need to present **target** by **context** means in a figure
 Report patterns 
 Report anova stats

#### ANOVA Results
We ran a 2 (**Target**) X 2 (**Context**) ANOVA

> Neal explains *degrees of freedom*: If you know the mean RT is 4150, and you know that one of the means is, for example, Q=4300, we can compute the other as 4000. 

* We did not find a ME effect of __Target__: letter (F(1,15) = 2.19, _p_ > .15)
* We did not find a ME effect of __Context__: (F(1,15) = 1.71, _p_ > .20)
* We _DID_ find an interaction between __Target__ and __Context__: (F(1,15) = 77.8, _p_ < .001)

```{r, echo=FALSE, error=FALSE, warning=FALSE}
library(ggplot2)

class.ano <- data.frame(
letter  = factor(c("Q","Q","Z","Z")),
context = factor(c("Rnd","Ang","Rnd","Ang"), levels=c("Rnd","Ang")),
RT    = c(4864, 3807, 3732, 4425))
```

```{r, echo=FALSE}

# Basic line graph with points
ano.pl <- ggplot(data=class.ano, aes(x=letter, y=RT, group=context, color=context)) +
    geom_line() +
    geom_point() +
    theme_bw() +
    scale_fill_hue(name="Context") +
    ggtitle("Reaction time as a function of context and target letter")
ano.pl + scale_shape_discrete(name="Context") + scale_color_discrete(name="Context")

```
### Row analyses

#### Collapsing

* Collapsed over repetition by taking means
* Collapsed over target and context by taking means 
    + this give us an overall R<sub>n</sub> mean for each person
* Collapsed over participants, giving us a set of row means

If people are paying attention to the instructions, there should be larger reaction times as the row number increases.

"Reaction Time by Row" plot
Reaction Time (ms)
Row


```{r, echo=FALSE, error=FALSE, warning=FALSE}

rt.by.row <- data.frame(
Row = c(1:20),
RT = c(2120,2011,2431,2545,3183,3126,3350,3499,3825,4217,4150,4672,4902,5007,5234,5775,5722,5936,6049,6021)
)
model <- lm(formula = RT ~ Row, data = rt.by.row)
eqn <- as.character( as.expression(
substitute(italic(y) == b * italic(x) + a * "," ~~ italic(r)^2 ~ "=" ~ r2 * "," ~~ italic(p) * "<.001",
           list(a = format(coef(model)[1], digits = 3),
                b = format(coef(model)[2], digits = 3),
                r2 = format(summary(model)$r.squared, digits=2)
                ))))
```


```{r, echo=FALSE}

pl<-ggplot(rt.by.row, aes(x = Row, y = RT))
pl + geom_point() + geom_smooth(method = lm, se = FALSE) +  theme_classic() + scale_x_continuous(breaks=c(1:20)) +
  annotate("text", label = eqn, parse = TRUE, x = Inf, y = -Inf, hjust = 1.1, vjust = -.5) +
  ggtitle("Reaction time by row") + xlab("Row") + ylab("RT (ms)")

```

We have a slope: y(RT) = 229x(row) + 1779

What does this tell us (that it take approximate. 229ms/row to scan one row)

You also want a Pearson R value:
* r=.99
* _p_<.001


 <div style="background:#fff0f5;border: 2px solid #990000">
## Results (15)

* dropped any trials where confirmation=<N>

1. factorial (target x context) analysis: 
  * collapsing (taking means) over row, repetition, and participants; 
  * describe pattern (Qrnd > Qang, Zrnd < Zang);
  * refer to fig; present ANOVA (no main eff of target, Q > Z (F(1,15)=2.19, p>.15); 
  * no main eff of context (F(1,15)=1.71, p>.20);
  * interaction, Qrnd>Qang, Zrnd<Zang: F(1,15)=77.8, p<.001)

2. row analysis: 
  * collapsing (taking means) over repetition, target, context, and participants; 
  * describe pattern (RT increasing with row);
  * refer to fig; present regression (slope = 229 ms/row, y-int = 1779 ms, r = 0.99, p < .001);
  * brief descrip of fit of line -- very good across the range [Spr2015 -- skipped this entirely; better for first 10 rows or so than for rest [could be here or in Discussion]]
(either order of above analyses is fine)
</div>
***
Feature Theory (Talked about in Intro):

* Knows and cares about _amount_ of overlap
* computes this overlap by comparing number of features shared (e.g., Q vs. D): (, ), ~ vs. ), |
* more shared features means slower reaction times
  + Q<sub>ROUND</sub> > Q<sub>ANGULAR</sub>
  + Z<sub>ROUND</sub> <= Z<sub>ANGULAR</sub>
  
# Discussion
* Template theory doesn't have a way of talking about anything other than overall overlap
* It doesn't say anything about combinations (It shouldn't know that a Q overlapped with a D has some overlap with a Q overlapped with a V)
* Predicts no interaction between target and context
* Specific pattern of results matches the feature theory predictions
* WE can define a specific set of features for the relevant letters, and we can define a function that computes amount of overlap
    + we have a set of features counted up (see table)

#### Calculating % overlap:    
 
Eq:  (% overlap = # features matching / # of features in context  
<div stlye="width:50%">
Target x context|Match/Total|Percentage
----------------|-----------|----------
Q<sub>ROUND</sub>| 2/5| 40
Q<sub>ANGULAR</sub>| 1/8 | 12.5
Z<sub>ROUND</sub> |0/5| 0
Z<sub>ANGULAR</sub>| 1/8 |12.5
</div>    
Because we don't see a perfect match in the computations and the results, there's more to the story 
-potential problem: we're not considering how many times each of the features appear, we might want to adjust the function to weight features

We found an interaction, which fits with the feature theory but does not fit with the template theory
-What might you do to the template theory to explain the pattern?
    

<div style="background:#fff0f5;border: 2px solid #990000">

## Discussion (15)

* compare results w/predictions (recap predictions if needed)
  * no effect of target or context, compatible w/either theory, but interaction inconsistent w/Template and consistent w/Feature; 
* explain meaning of slope (time to scan a row) [and mention goodness of fit to regression line for earlier vs later rows (optional; should be here or in Results; nothing to do for Spr2015)];

* discuss augmentation of Template;

* discuss Structural Feature Theory (if not covered in Intro);

* cover detailed feature theory and quantitative predictions here, relating to ANOVA results, etc. (some flexibility about how much they say about quantitative feature predictions, but they should present a feature-breakdown and work out % of overlap, relating to results, at a minimum); 

* usual general issues & extensions


## FIGS AND TABLES (15)

graph for RT by target and context

graph for RT by row, including regression line

check axis labels, axis tick spacing, graph titles, etc.

</div>



***
### READINGS:
* Levy, C.M., & Ransdell, S. E. (1988). _Laboratory in cognition and perception (2nd ed.)._ Iowa City: CONDUIT, Univ. of Iowa.
* Neisser, U. (1967) _Cognitive Psychology_. Englewood Cliffs, NJ: Prentice-Hall. [Neisser (1967)](http://infinitychallenge.com/articles/Neisser_1967.pdf)
* Reed, S.K. (1998) _Cognition: Theory and appliciations_. Pacific Grove, CA: Brooks/Cole. [Reed (1988)](http://infinitychallenge.com/articles/Reed_1988.pdf)
* Schneider, W. (1988) Micro Experimental Laboratory: An integrated system for IBM PC compatibles: _Behavior Research Methods, Instruments, & Computers, 20_, 206-217.

```{r, echo=FALSE, eval=FALSE}
library(languageR) 
library(reshape2)
library(plyr)
library(stringr)
# target: 1=Q, 2=Z
# context: 1=rnd, 2=ang
# row: 1-20
# Rep
# 1\2\12\1
class.dat.base <- read.table("data/EXP3.txt", header = T) 
colnames(class.dat.base) <- c("subj", "trial", "target", "context", "row", "rep","RT","OK")
class.dat    <- subset(class.dat.base, target != 1 & context != 2 & row != 12 & rep != 1 & OK != 2)


# # CAT: Collapse over subject
# d.cat$unique.id <- paste(d.cat[, 1], d.cat[, 3], d.cat[, 4], d.cat[, 5], sep = "_")
# d.cat           <- ddply(d.cat, "unique.id", function(X) data.frame(vtime = mean(X$vtime)))
# vtime           <- data.frame(d.cat$vtime)
# d.cat           <- colsplit(d.cat$unique.id, "_", c("subj", "subexp", "headrel", "localrel"))
# d.cat$subj      <- as.factor(d.cat$subj)
# d.cat           <- cbind(d.cat,vtime)
# colnames(d.cat)[5] <- "vtime"
# rm(vtime)
```

